# Developer-Foundations-Capstone

## Summary 
This capstone project serves to validate an individual’s readiness for future classes such as the course Data Engineering with Databricks and Scalable Machine Learning on Databricks.

## Description
The Apache Spark Programming Capstone helps students put into practice skills learned in its prerequisite course, Apache Spark Programming with Databricks. This capstone also serves to validate participants’ readiness for the next courses in this series of partners’ classes, Data Engineering with Databricks and Machine Learning on Databricks.

In this capstone project, participants will engage in very simple ETL operations: to read data from multiple data sources; to produce new datasets by unioning and joining the source datasets; to employing various, prescribed transformations; to load the datasets back to disk in various forms. In addition to this, the participant will also leverage the Delta and Structured Streaming APIs to create a simple, streaming, pipeline as prescribed in the capstone project.

Participants are not expected to have an in-depth knowledge of the Spark and DataFrames APIs but should have some familiarity with the Spark APIs (SparkContext, SparkSession, DataFrame, DataFrameReaders, DataFrameWriters, Row and Column), architectural concepts, and should be able to navigate the Spark docs to lookup how to employ various transformations and actions. In addition to the core spark components, candidates should have a working knowledge of the Structured Streaming and Delta APIs.

## Learning objectives
* This capstone will evaluate the learner’s ability to:
* Apply transformations and actions as prescribed in the project
* Employ the DataFrameReaders to ingest JSON, CSV, XML, Parquet, and Delta datasets
* Employ the DataFrameWriters to load data into Parquet and Delta tables with prescribed features
* Develop a Structured Stream job to ingest data and merge it with a Delta table
* Update configuration settings to control and tune the application
* Extract data from a DataFrame to answer simple business questions

## Prerequisites
* Intermediate to advanced experience with Python
* Introductory knowledge of the Spark APIs (SparkContext, SparkSession, DataFrame, DataFrameReader, DataFrameWriter, Row, Column)
* Introductory knowledge of the Spark Architecture

## Getting Started

### Overview
There are two ways to get stareted with this project:
1. Importing our GitHub repo into your Databricks workspace using the new **Repos** feature.
2. Via a direct link to a DBC file, importing the notebooks directly it into your Databricks workspace.

Both procedures are documented for you at <a href="https://www.databricks.training/step-by-step/importing-courseware-from-github" target="_blank">https&#58;//www.databricks.training/step-by-step/importing-courseware-from-github</a>.<br/>
Quick hint: Open this page in a new tab.

### Completing the Capstone Project
1. Start with notebook **Exercise 01 - Overview and Install** which provides an overview and context for the project.
2. From there, work your way through each notebook (1-7)
3. Instructions on submitting your work is included in the notebook **Exercise 07 - Submittsion**
